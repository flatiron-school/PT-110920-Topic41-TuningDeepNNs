{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic 41: Deep Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First: Let's Explore\n",
    "\n",
    "https://playground.tensorflow.org/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Tuning Neural Networks\n",
    "\n",
    "<img src=\"https://media.giphy.com/media/st83jeYy9L6Bq/giphy.gif\">\n",
    "\n",
    "### AKA - Huge Blocks of Text, For Reference!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a. Adding nodes and layers\n",
    "\n",
    "- Number of hidden layers\n",
    "\n",
    "*For many problems you can start with just one or two hidden layers it will work just fine. For more complex problems, you can gradually ramp up the number of hidden layers until your model starts to over fit. Very complex tasks, like image classification without convolutional layers, will need dozens of layers.* \n",
    "\n",
    "- Number of neurons per layer\n",
    "\n",
    "*The number of neurons for the input and output layers is dependent on your data and the task. i.e. input dimensions are determined by your number or columns and your output layer for binary classification has just one node with a sigmoid activation function. For hidden layers, a common practice is to create a funnel with funnel with fewer and fewer neurons per layer.*\n",
    "\n",
    "*In general, you will get more bang for your buck by adding on more layers than adding more neurons.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. Activation functions\n",
    "\n",
    "* One thing important for activation functions is its **differentiability** because the derivative is used in the backpropagation process\n",
    "\n",
    "<img src='images/activation.png' width=500/>\n",
    "\n",
    "#### Activation functions for output layers (for supervised learning problems)\n",
    "\n",
    "1. For binary classification problems: sigmoid activation to coerce values between 0-1\n",
    "2. For multiclass classification: softmax activation, as it produces a non-negative vector that sums to 1 (probabilities of your test point belonging to the different classes)\n",
    "3. For regression problems: linear, or relu activation (it is linear and unbounded!)\n",
    "\n",
    "#### Activation functions for hidden layers\n",
    "\n",
    "Relevant/useful blog posts (by different authors, despite the similar titles): \n",
    "- [Exploring Activation Functions for Neural Networks](https://towardsdatascience.com/exploring-activation-functions-for-neural-networks-73498da59b02)\n",
    "    - Discusses why adding non-linear activation functions helps us solve non-linear problems, as well as why an understanding of the derivatives of these activation functions helps us tune NNs more effectively\n",
    "- [Understanding Activation Functions in Neural Networks](https://medium.com/the-theory-of-everything/understanding-activation-functions-in-neural-networks-9491262884e0)\n",
    "    - Builds the intuitition behind activation functions (despite less-than-stellar grammar and punctuation throughout)\n",
    "\n",
    "Notes: \n",
    "* Sigmoids are not used often because it has a small maximum derivative value and thus propagates only a small amount of error each time, leading to slow \"learning\" \n",
    "* This small-derivative-slow-learning issue is known as a **vanishing gradient** problem\n",
    "* Tanh is mathematically quite similar to a sigmoid function, thus also has the vanishing gradient issue, but not as bad\n",
    "* ReLu generally works well because its gradient is always 1, as long as the input is positive (no vanishing gradients), and negative inputs going to 0 can make your network lighter (no weights/biases are being updated)\n",
    "\n",
    "Note - you may want to consider both the [vanishing gradient](https://en.wikipedia.org/wiki/Vanishing_gradient_problem) and [exploding gradient](https://machinelearningmastery.com/exploding-gradients-in-neural-networks/) problems. And here's a resource on fixing the vanishing gradient problem: https://machinelearningmastery.com/how-to-fix-vanishing-gradients-using-the-rectified-linear-activation-function/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c. Loss functions\n",
    "\n",
    "Loss functions are akin to cost functions we were trying to minimize in gradient descent (i.e. RMSE for linear regression, Gini/entropy for trees)\n",
    "\n",
    "1. For regression problems, keras has **mean_squared_error** or **mean_absolute_error** as a loss function, or **mean_squared_logarithmic_error** if your target has potential outliers\n",
    "2. For binary classification: **binary_crossentropy** \n",
    "3. For multiclass problems: **categorical_crossentropy**\n",
    "\n",
    "[This article summarizes the above, and more.](https://machinelearningmastery.com/how-to-choose-loss-functions-when-training-deep-learning-neural-networks/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d. Optimizers\n",
    "\n",
    "Overview Resource: \n",
    "- [A (Quick) Guide to Neural Network Optimizers with Applications in Keras](https://towardsdatascience.com/a-quick-guide-to-neural-network-optimizers-with-applications-in-keras-e4635dd1cca4) blog post - an overview of the options\n",
    "\n",
    "Summary:\n",
    "* Different optimizers are just different methods/paths that your neural network can take to find optimal values\n",
    "* Experimentally, Adam (derived from *adaptive moment estimation*) is a good one to use ([more about Adam](https://machinelearningmastery.com/adam-optimization-algorithm-for-deep-learning/))\n",
    "\n",
    "#### Quick optimizer summary\n",
    "\n",
    "* **RMSProp**: maintains per-parameter learning rates adapted based on the average of recent weight updates (e.g. how quickly it is changing). This does well on non-stationary problems (e.g. noisy data)\n",
    "\n",
    "* **Adagrad**: maintains a per-parameter learning rate that improves performance on problems with sparse gradients (e.g. natural language and computer vision problems)\n",
    "\n",
    "* **Adam**: realizes the benefits of both AdaGrad and RMSProp. Instead of adapting the parameter learning rates based on the average first moment (the mean) as in RMSProp, Adam also makes use of the average of the second moments of the gradients (the uncentered variance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### e. Learning rate\n",
    "\n",
    "* The learning rate is something you can define when you compile your model with the optimizer\n",
    "* Optimizers usually change up the learning rates, so this is just the *initial* learning rate\n",
    "* If you set it too low, training will eventually converge, but it will do so slowly\n",
    "* If you set it too high, it might actually diverge\n",
    "* If you set it slightly too high, it will converge at first but miss the local optima"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### f. Regularization\n",
    "\n",
    "* As a neural network learns, neuron weights settle into their context within the network\n",
    "* Weights of neurons are tuned for specific features providing some specialization\n",
    "* Neighboring neurons become too reliant on this specialization, which if taken too far can result in a fragile model too specialized to the training data\n",
    "* This reliance on context for a neuron during training is referred to as *complex co-adaptations*\n",
    "\n",
    "#### Methods\n",
    "1. You can add L1 or L2 regularization within each hidden layer\n",
    "2. You can also add a **dropout layer** \n",
    "3. Not technically *regularization*, but you can introduce **early stopping** so your model doesn't overtrain\n",
    "\n",
    "#### Dropout\n",
    "Dropout is a technique where randomly selected neurons are ignored during training. They are “dropped-out” randomly. This means that their contribution to the activation of downstream neurons is temporally removed on the forward pass and any weight updates are not applied to the neuron on the backward pass. You can add **dropout layers** in your neural network.\n",
    "\n",
    "<img src='images/thanos.png'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create, then Tune!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from keras.datasets import mnist\n",
    "\n",
    "# Note - importing keras straight from tensorflow here\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_training_results(history):\n",
    "    '''\n",
    "    From https://machinelearningmastery.com/display-deep-learning-model-training-history-in-keras/\n",
    "    \n",
    "    Input: keras history object (output from trained model)\n",
    "    '''\n",
    "    fig, (ax1, ax2) = plt.subplots(2, sharex=True)\n",
    "    fig.suptitle('Model Results')\n",
    "\n",
    "    # summarize history for accuracy\n",
    "    ax1.plot(history.history['accuracy'])\n",
    "    ax1.plot(history.history['val_accuracy'])\n",
    "    ax1.set_ylabel('Accuracy')\n",
    "    ax1.legend(['train', 'test'], loc='upper left')\n",
    "    # summarize history for loss\n",
    "    ax2.plot(history.history['loss'])\n",
    "    ax2.plot(history.history['val_loss'])\n",
    "    ax2.set_ylabel('Loss')\n",
    "    ax2.legend(['train', 'test'], loc='upper left')\n",
    "    \n",
    "    plt.xlabel('Epoch')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explore Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grabbing our data directly from keras\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore an example\n",
    "sample_index = 42\n",
    "\n",
    "sample_image = X_train[sample_index]\n",
    "sample_label = y_train[sample_index]\n",
    "display(plt.imshow(sample_image))\n",
    "print('Label: {}'.format(sample_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_image.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prep Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize our inputs - from https://keras.io/examples/vision/mnist_convnet/\n",
    "input_shape = (28, 28, 1) # creating this tuple for later\n",
    "\n",
    "# Scale images to the [0, 1] range\n",
    "X_train = X_train.astype(\"float32\") / 255\n",
    "X_test = X_test.astype(\"float32\") / 255\n",
    "# Make sure images have shape (28, 28, 1)\n",
    "X_train = np.expand_dims(X_train, -1)\n",
    "X_test = np.expand_dims(X_test, -1)\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(X_train.shape[0], \"train samples\")\n",
    "print(X_test.shape[0], \"test samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare our outputs\n",
    "num_classes = 10\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"y_train shape:\", y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First S***** Model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model building in a single list\n",
    "model = keras.Sequential(\n",
    "    [\n",
    "        None\n",
    "    ])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining our training hyperparameters\n",
    "# Initial values from: https://keras.io/examples/vision/mnist_convnet/\n",
    "batch_size = 128\n",
    "epochs = 15\n",
    "\n",
    "# Compiling our model\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit our model (save output to a history variable)\n",
    "history = model.fit(X_train, \n",
    "                    y_train, \n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs, \n",
    "                    validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate!\n",
    "score = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Test loss:\", score[0])\n",
    "print(\"Test accuracy:\", score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize results\n",
    "visualize_training_results(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate: \n",
    "\n",
    "How'd we do?\n",
    "\n",
    "- \n",
    "\n",
    "\n",
    "What should we do next?\n",
    "\n",
    "- \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Next!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## If We Have Time: \n",
    "\n",
    "# Building a CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A convolutional neural network is a neural network with **convolutional layers**. CNNs are mainly used for image recognition/classification. They can be used for video analysis, NLP (sentiment analysis, topic modeling), and speech recognition. \n",
    "\n",
    "### How do our brains see an image? \n",
    "\n",
    "We might see some fluffy tail, a wet nose, flappy ears, and a good boy and conclude we are probably seeing a dog. There is not one singular thing about a dog that our brain recognizes as a dog but an amalgamation of different patterns that allow us to make a probable guess.  \n",
    "\n",
    "<img src='images/chihuahua.jpeg'/>\n",
    "\n",
    "### How do computers see images?\n",
    "\n",
    "<img src='images/architecture.jpeg' width=700/>\n",
    "\n",
    "To computers, color images are a 3D object - composed of 3 matrices - one for each primary color that can be combined in varying intensities to create different colors. Each element in a matrix represents the location of a pixel and contains a number between 0 and 255 which indicates the intensity of the corresponding primary color in that pixel.\n",
    "\n",
    "<img src='images/rgb.png'/>\n",
    "\n",
    "## Convolutions\n",
    "\n",
    "**To *convolve* means to roll together**. CNNs make use of linear algebra to identify patterns using the pixel values (intensity of R,G, or B). By **taking a small matrix and moving it across an image and multiplying them together every time it moves**, our network can mathematically identify patterns in these images. This small matrix is known as a *kernel* or *filter* and each one is designed to identify a particular pattern in an image (edges, shapes, etc.)\n",
    "\n",
    "<img src='images/convolve.gif' width=500/>\n",
    "\n",
    "When a filter is \"rolled over\" an image, the resulting matrix is called a **feature map** - literally a map of where each pattern of feature is in the image. Elements with higher values indicate the presence of a pattern the filter is looking for. The values (or weights) of the filter are adjusted during back-propagation.\n",
    "\n",
    "#### Convolutional layer parameters\n",
    "\n",
    "1. Padding: sometimes it is convenient to pad the input volume with zeros around the border. Helps with detecting patterns at the edge of an image\n",
    "2. Stride: the number of pixels to shift the filter on each \"roll\". The larger the stride, the smaller the feature map will be - but we will lose more information\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pooling Layers\n",
    "\n",
    "After a convolutional layer, the feature maps are fed into a max pool layer. Like convolutions, this method is applied one patch at a time (usually 2x2). Max pooling simply takes the largest value from one patch of an image, places it in a new matrix next to the max values from other patches, and discards the rest of the information contained in the activation maps. Other methods exist such as average pooling (taking an average of the patch).\n",
    "\n",
    "<img src='images/maxpool.png'/>\n",
    "\n",
    "This process results in a new feature map with reduced dimensionality that is then passed into another convolution layer to continue the pattern finding process. These steps are repeated until they are passed to a fully connected layer that proceeds to classify the image using the identified patterns.\n",
    "\n",
    "### Flattening\n",
    "\n",
    "Once the neural network has collected a series of patterns that an image contains, it is ready to make a guess as to what the image is. In order to do so, it starts by **flattening** the 2D matrix into a 1D vector, so it can be passed into a normal densely connected layer for classification. Then using this vector, one or many densely connected layers will make a prediction as to what the image is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New dataset!\n",
    "from keras.datasets import cifar10\n",
    "\n",
    "# load dataset\n",
    "(X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
    "# summarize loaded dataset\n",
    "print('Train: X=%s, y=%s' % (X_train.shape, y_train.shape))\n",
    "print('Test: X=%s, y=%s' % (X_test.shape, y_test.shape))\n",
    "# plot first few images\n",
    "for i in range(9):\n",
    "    # define subplot\n",
    "    plt.subplot(330 + 1 + i)\n",
    "    # plot raw pixel data\n",
    "    plt.imshow(X_train[i], cmap=plt.get_cmap('gray'))\n",
    "    \n",
    "plt.show()\n",
    "# If you get a \"downloading data\" thing, worry not, shouldn't take long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the class balance of our data\n",
    "pd.DataFrame(y_train)[0].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Still need to prep our data\n",
    "# Scale images to the [0, 1] range\n",
    "X_train = X_train.astype(\"float32\") / 255\n",
    "X_test = X_test.astype(\"float32\") / 255\n",
    "# Make sure images have shape (32, 32, 3)\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(X_train.shape[0], \"train samples\")\n",
    "print(X_test.shape[0], \"test samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And, prep our outputs\n",
    "num_classes = 10\n",
    "\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model building using \"add\"\n",
    "# Model architecture from MLM post linked below\n",
    "cnn = keras.Sequential()\n",
    "cnn.add(layers.Conv2D(32, (3, 3), activation='relu', padding='same', input_shape=(32, 32, 3)))\n",
    "cnn.add(layers.Conv2D(32, (3, 3), activation='relu', padding='same'))\n",
    "cnn.add(layers.MaxPooling2D((2, 2)))\n",
    "cnn.add(layers.Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
    "cnn.add(layers.Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
    "cnn.add(layers.MaxPooling2D((2, 2)))\n",
    "cnn.add(layers.Conv2D(128, (3, 3), activation='relu', padding='same'))\n",
    "cnn.add(layers.Conv2D(128, (3, 3), activation='relu', padding='same'))\n",
    "cnn.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "# now, to get the proper output\n",
    "cnn.add(layers.Flatten())\n",
    "cnn.add(layers.Dense(128, activation='relu', kernel_initializer='he_uniform'))\n",
    "cnn.add(layers.Dense(10, activation='softmax'))\n",
    "\n",
    "cnn.compile(loss='categorical_crossentropy',\n",
    "            optimizer=\"adam\",\n",
    "            metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = cnn.fit(X_train,\n",
    "                  y_train,\n",
    "                  epochs=5, # small since this is just a demo\n",
    "                  batch_size=64,\n",
    "                  validation_data=(X_test, y_test))\n",
    "# note - even with just 5 epochs this'll take many minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate!\n",
    "score = cnn.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Test loss:\", score[0])\n",
    "print(\"Test accuracy:\", score[1])\n",
    "\n",
    "# Visualize results\n",
    "visualize_training_results(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A worked-through example on this dataset: https://machinelearningmastery.com/how-to-develop-a-cnn-from-scratch-for-cifar-10-photo-classification/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "174.46px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
